---
title: "ST558 Final Project by Wenna Han - Modeling"
format: html
editor: visual
---

## Introduction

In this study, we utilize the diabetes_binary_health_indicators_BRFSS2015.csv data (from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) to investigate the relationships between diabetes status and a range of health-related variables.

The dataset comprises 218,334 individuals without diabetes and 35,346 individuals with diabetes, encompassing various demographic, behavioral, and health status indicators. The response variable is Diabetes_binary, which represent whether a person has diabetes or not. The data has other 21 variables. Based on the exploratory data analysis (EDA) results, the following variables are identified as potential predictors of Diabetes_binary:\
- **High Blood Pressure (HighBP)**: whether an individual has high blood pressure.\
- **High Cholesterol (HighChol)**: whether an individual has high cholesterol.\
- **Body Mass Index (BMI)**: body mass index\
- **Stroke**: whether an individual has ever had a stroke.\
- **Heart Disease or Attack (HeartDiseaseorAttack)**: whether an individual has ever had heart disease or a heart attack.\
- **Physical Activity (PhysActivity)**: whether an individual has engaged in physical activity in the past month.\
- **Fruits Consumption (Fruits)**: whether an individual consumes fruits daily.\
- **Vegetables Consumption (Veggies)**: whether an individual consumes vegetables daily.\
- **Heavy Alcohol Consumption (HvyAlcoholConsump)**: whether an individual engages in heavy alcohol consumption.\
- **General Health (GenHlth)**: general health condition.\
- **Mental Health (MentHlth)**: mental health condition.\
- **Physical Health (PhysHlth)**: phsical health condition.\
- **Difficulty Walking (DiffWalk)**: whether an individual has difficulty walking or climbing stairs.\
- **Age**\
- **Education**\
- **Income**

The goal of this project is to develop a predictive model that can accurately classify individuals as having diabetes or not based on their health and demographic profiles. This model can be valuable for early detection and prevention strategies, allowing healthcare providers to identify at-risk individuals and implement targeted interventions to manage or mitigate the risk of diabetes.

## load necessary packages

```{r}
library(tidyverse)
library(caret)
library(dplyr)
library(glmnet)
library(ranger)
library(rpart)
library(randomForest)
```

## Prepare data

```{r}
# read data with relative path
data <- read.csv("./diabetes_binary_health_indicators_BRFSS2015.csv") 

# select variables that should be converted to 0/1 coded factors
No_factor<-c(1:4, 6:14, 18) 

# Define the levels and labels
levels_0_1 <- c(0, 1)
labels_no_yes <- c("no", "yes")

# Convert the selected variables to factors with levels 0 and 1 and labels No and Yes
data[, No_factor] <- lapply(data[, No_factor], function(x) {
  factor(x, levels = levels_0_1, labels = labels_no_yes)
})

# Extract data for modeling
data <- data |>
  select(-CholCheck, -Smoker, -AnyHealthcare, -NoDocbcCost, -Sex)

# check data structure
str(data)
```

## Split data 70/30

```{r}
set.seed(5580728)

trainIndex <- createDataPartition(data$Diabetes_binary, p = .7,
                                  list = FALSE,
                                  times = 1)

train_set <-  data[trainIndex, ]
test_set <- data[-trainIndex, ]

#check data
dim(train_set)
dim(test_set)
```

## Use logLoss as the metric to evaluate models

Log Loss is a performance metric for evaluating the accuracy of a classification model where the outcome is a probability value between 0 and 1. Specifically, for binary classification problems, Log Loss quantifies the uncertainty of the predictions made by the model. It penalizes false classifications, with a higher penalty for predictions that are confident but wrong.\
We may prefer Log Loss over accuracy when we have a binary response variable since that accuracy only considers whether the prediction was correct or not, without accounting for the confidence of the prediction. Log Loss, on the other hand, takes into account the probability assigned to each class, penalizing confident incorrect predictions more than less confident ones. Also, in datasets with imbalanced classes, a high accuracy can be misleading if the model is biased towards the majority class. Log Loss mitigates this by penalizing misclassifications based on the predicted probabilities, providing a more accurate assessment of model performance across both classes.

## Implementation of Log Loss with 5-Fold Cross-Validation
In our study, we will use Log Loss with 5-fold cross-validation to evaluate and select the best predictive model for the Diabetes_binary variable. Cross-validation ensures that our model generalizes well to unseen data by assessing its performance on different subsets of the dataset. By setting up a grid of tuning parameters for each model, we aim to optimize the model's hyperparameters to achieve the lowest possible Log Loss, ensuring robust and reliable predictions.
```{r}
trctrl <- trainControl(method = "cv", 
                       number = 5, 
                       summaryFunction = mnLogLoss, 
                       classProbs = TRUE)
```

## Logistic Regression Models
Logistic Regression is used for modeling a binary response variable, which means the dependent variable has two possible outcomes. Unlike linear regression, which predicts continuous outcomes, logistic regression predicts the probability of an event occurring. 
The logistic regression model estimates the probability p that an outcome Y is equal to 1 given a set of predictor variables X.\
We choose it because logistic regression is designed for binary outcomes, making it ideal for our goal of predicting diabetes status (yes/no). The coefficients in logistic regression can be interpreted as the log odds of the outcome, providing insights into the relationship between predictor variables and the likelihood of diabetes. Also, it can handle both continuous and categorical predictor variables and can be extended to multi-class classification problems.

### Model 1: Basic Logistic Regression with all identified potential predictors
```{r}
# Model 1: Basic Logistic Regression with all identified potential predictors
set.seed(123)
logit_model1 <- train(Diabetes_binary ~ .,
                      data = train_set,
                      method = "glm",
                      family = binomial,
                      trControl = trctrl,
                      metric = "logLoss"
)
```

### Model 2: Logistic Regression with Stepwise Selection
Use the stepwise selection to select predictors from the long list. 
```{r}
# Model 2: Logistic Regression with Stepwise Selection
set.seed(123)
logit_model2 <- train(Diabetes_binary ~ .,
                      data = train_set,
                      method = "glmStepAIC",
                      family = binomial,
                      trControl = trctrl,
                      metric = "logLoss"
)
```

### Model 3: Logistic Regression with interaction Terms
Based on EDA results, GenHealth is highly correlated with MentHlth as well as PhysHlth. Also, MentHlth and PhysHlth are correlated. Thus, in model 3, we removed the GenHealth and include teh interaction between MentHlth and PhysHlth.
```{r}
# Model 3: Logistic Regression with interaction Terms
set.seed(123)
logit_model3 <- train(Diabetes_binary ~ HighBP + HighChol +  BMI + 
                        Stroke + HeartDiseaseorAttack + PhysActivity +
                        Fruits + Veggies + HvyAlcoholConsump + 
                        MentHlth*PhysHlth +  DiffWalk + Age + Education + Income,
                      data = train_set,
                      method = "glm",
                      family = binomial,
                      trControl = trctrl,
                      metric = "logLoss"
)
```

### Compare models
```{r}
# Summarize the results
results <- resamples(list(
  Model1 = logit_model1,
  Model2 = logit_model2,
  Model3 = logit_model3
))

summary(results)
```
Model1 has the lowest log-loss, thus, we select logit_model2, which is:
```{r}
summary(logit_model2)
```

## Classification Tree Models
A classification tree is a type of decision tree used for categorizing a dependent variable into distinct classes. It is a supervised learning technique that partitions the dataset into subsets based on the value of input features, creating a tree-like model of decisions. Each internal node in the tree represents a "test" on an attribute (e.g., whether an attribute is less than or greater than a certain value), each branch represents the outcome of the test, and each leaf node represents a class label (decision) or distribution of class labels.\
We try classification trees because trees are easy to understand and visualize, making them useful for explaining model decisions. Also, classification trees can capture non-linear relationships between features and the target variable. It do not require normalization or standardization of features. Moreover, it can help identify the most important features in the dataset. 
```{r}
# classification tree
set.seed(123)
classification_tree_model <- train(Diabetes_binary ~ .,
                            data = train_set,
                            method = "rpart",
                            trControl = trctrl,
                            tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)),
                            metric = "logLoss")
classification_tree_model
```
logLoss was used to select the optimal model using the smallest value. The final value used for the model was cp = 0.001.

## Random Forest
A random forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is an extension of bagging (Bootstrap Aggregating) and leverages the power of multiple models to improve accuracy and control over-fitting.\
We want to try random forest model because it can achieve better accuracy than individual decision trees by averaging multiple trees. Also, random forests are less likely to overfit compared to individual trees.
```{r}
set.seed(123)
random_forest_model <- train(Diabetes_binary ~ .,
                             data = train_set,
                             method = "ranger",
                             trControl = trctrl,
                             tuneGrid = expand.grid(mtry = c(2, 4, 6),
                                                    splitrule = "extratrees",
                                                    min.node.size = c(1, 5, 10)),
                             metric = "logLoss",
                             num.trees = 100)
random_forest_model
```
logLoss was used to select the optimal model using the smallest value. The final values used for the model were mtry = 4, splitrule = extratrees and min.node.size = 10.

## Final Model Selection
Now we have three best models (logit_model1, classification_tree_model, random_forest_model). Now compare all three models on the test set and declare an overall winner.
```{r}
# Predict probabilities for the test set
logit_probs <- predict(logit_model1, newdata = test_set, type = "prob")
tree_probs <- predict(classification_tree_model, newdata = test_set, type = "prob")
rf_probs <- predict(random_forest_model, newdata = test_set, type = "prob")

# Define a function to calculate log loss
logLoss <- function(pred_probs, actual) {
  actual <- ifelse(actual == levels(actual)[1], 0, 1)
  -mean(actual * log(pred_probs[, 2]) + (1 - actual) * log(pred_probs[, 1]))
}

# Calculate log loss for each model
logit_log_loss <- logLoss(logit_probs, test_set$Diabetes_binary)
tree_log_loss <- logLoss(tree_probs, test_set$Diabetes_binary)
rf_log_loss <- logLoss(rf_probs, test_set$Diabetes_binary)

# Print log loss values
list(Logit_LogLoss = logit_log_loss,
     Classification_Tree_LogLoss = tree_log_loss,
     Random_Forest_LogLoss = rf_log_loss)
```
The random forest model has the smallest log loss, thus, the **random forest model** is the winner!


