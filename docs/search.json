[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 Final Project by Wenna Han - EDA",
    "section": "",
    "text": "In this study, we utilize the diabetes_binary_health_indicators_BRFSS2015.csv data (from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) to investigate the relationships between diabetes status and a range of health-related variables.\nThe dataset comprises 218,334 individuals without diabetes and 35,346 individuals with diabetes, encompassing various demographic, behavioral, and health status indicators. The response variable is Diabetes_binary, which represent whether a person has diabetes or not. The data has other 21 variables as potential predictors. For example, whether an individual has high blood pressure (HighBP), whether an individual has high cholesterol (HighChol), whether an individual has ever had their cholesterol checked (CholCheck), Body Mass Index (BMI), whether an individual is a current smoker (Smoker), whether an individual has ever had a stroke (Stroke), whether an individual has ever had heart disease or a heart attack (HeartDiseaseorAttack), whether an individual has engaged in physical activity in the past month (PhysActivity), whether an individual consumes fruits daily (Fruits), whether an individual consumes vegetables daily (Veggies), whether an individual engages in heavy alcohol consumption (HvyAlcoholConsump), whether an individual has access to healthcare coverage (AnyHealthcare), whether cost has prevented an individual from visiting a doctor in the past year (NoDocbcCost), general health (GenHlth), mental health (MentHlth), physical health (PhysHlth), whether an individual has difficulty walking or climbing stairs (DiffWalk), age, sex, income, and education level.\nThe primary purpose of the exploratory data analysis is to uncover patterns, relationships, and potential factors associated with diabetes. By examining the distribution of these variables and their relationships with diabetes status, we aim to identify key indicators that might contribute to the presence of diabetes. The ultimate goal of our modeling efforts is to develop a predictive model that can accurately classify individuals as having diabetes or not based on their health and demographic profiles. This model can be valuable for early detection and prevention strategies, allowing healthcare providers to identify at-risk individuals and implement targeted interventions to manage or mitigate the risk of diabetes."
  },
  {
    "objectID": "EDA.html#st558-final-project---wenna-han",
    "href": "EDA.html#st558-final-project---wenna-han",
    "title": "EDA",
    "section": "",
    "text": "For this project, we will analyze a Diabetes Health Indicators Dataset: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset. Specifically, the diabetes_binary_health_indicators_BRFSS2015.csv data."
  },
  {
    "objectID": "EDA.html#read-in-data",
    "href": "EDA.html#read-in-data",
    "title": "EDA",
    "section": "Read In data",
    "text": "Read In data\nYou should read about the data and especially about each variable. Many are coded numerically but most are not numeric variables.\n\ndata &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\nhead(data)\n\n  Diabetes_binary HighBP HighChol CholCheck BMI Smoker Stroke\n1               0      1        1         1  40      1      0\n2               0      0        0         0  25      1      0\n3               0      1        1         1  28      0      0\n4               0      1        0         1  27      0      0\n5               0      1        1         1  24      0      0\n6               0      1        1         1  25      1      0\n  HeartDiseaseorAttack PhysActivity Fruits Veggies HvyAlcoholConsump\n1                    0            0      0       1                 0\n2                    0            1      0       0                 0\n3                    0            0      1       0                 0\n4                    0            1      1       1                 0\n5                    0            1      1       1                 0\n6                    0            1      1       1                 0\n  AnyHealthcare NoDocbcCost GenHlth MentHlth PhysHlth DiffWalk Sex Age\n1             1           0       5       18       15        1   0   9\n2             0           1       3        0        0        0   0   7\n3             1           1       5       30       30        1   0   9\n4             1           0       2        0        0        0   0  11\n5             1           0       2        3        0        0   0  11\n6             1           0       2        0        2        0   1  10\n  Education Income\n1         4      3\n2         6      1\n3         4      8\n4         3      6\n5         5      4\n6         6      8\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "",
    "text": "In this study, we utilize the diabetes_binary_health_indicators_BRFSS2015.csv data (from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) to investigate the relationships between diabetes status and a range of health-related variables.\nThe dataset comprises 218,334 individuals without diabetes and 35,346 individuals with diabetes, encompassing various demographic, behavioral, and health status indicators. The response variable is Diabetes_binary, which represent whether a person has diabetes or not. The data has other 21 variables. Based on the exploratory data analysis (EDA) results, the following variables are identified as potential predictors of Diabetes_binary:\n- High Blood Pressure (HighBP): whether an individual has high blood pressure.\n- High Cholesterol (HighChol): whether an individual has high cholesterol.\n- Body Mass Index (BMI): body mass index\n- Stroke: whether an individual has ever had a stroke.\n- Heart Disease or Attack (HeartDiseaseorAttack): whether an individual has ever had heart disease or a heart attack.\n- Physical Activity (PhysActivity): whether an individual has engaged in physical activity in the past month.\n- Fruits Consumption (Fruits): whether an individual consumes fruits daily.\n- Vegetables Consumption (Veggies): whether an individual consumes vegetables daily.\n- Heavy Alcohol Consumption (HvyAlcoholConsump): whether an individual engages in heavy alcohol consumption.\n- General Health (GenHlth): general health condition.\n- Mental Health (MentHlth): mental health condition.\n- Physical Health (PhysHlth): phsical health condition.\n- Difficulty Walking (DiffWalk): whether an individual has difficulty walking or climbing stairs.\n- Age\n- Education\n- Income\nThe goal of this project is to develop a predictive model that can accurately classify individuals as having diabetes or not based on their health and demographic profiles. This model can be valuable for early detection and prevention strategies, allowing healthcare providers to identify at-risk individuals and implement targeted interventions to manage or mitigate the risk of diabetes."
  },
  {
    "objectID": "Modeling.html#quarto",
    "href": "Modeling.html#quarto",
    "title": "Modeling",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Modeling.html#running-code",
    "href": "Modeling.html#running-code",
    "title": "Modeling",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "ST558 Final Project by Wenna Han - EDA",
    "section": "",
    "text": "In this study, we utilize the diabetes_binary_health_indicators_BRFSS2015.csv data (from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) to investigate the relationships between diabetes status and a range of health-related variables.\nThe dataset comprises 218,334 individuals without diabetes and 35,346 individuals with diabetes, encompassing various demographic, behavioral, and health status indicators. The response variable is Diabetes_binary, which represent whether a person has diabetes or not. The data has other 21 variables as potential predictors. For example, whether an individual has high blood pressure (HighBP), whether an individual has high cholesterol (HighChol), whether an individual has ever had their cholesterol checked (CholCheck), Body Mass Index (BMI), whether an individual is a current smoker (Smoker), whether an individual has ever had a stroke (Stroke), whether an individual has ever had heart disease or a heart attack (HeartDiseaseorAttack), whether an individual has engaged in physical activity in the past month (PhysActivity), whether an individual consumes fruits daily (Fruits), whether an individual consumes vegetables daily (Veggies), whether an individual engages in heavy alcohol consumption (HvyAlcoholConsump), whether an individual has access to healthcare coverage (AnyHealthcare), whether cost has prevented an individual from visiting a doctor in the past year (NoDocbcCost), general health (GenHlth), mental health (MentHlth), physical health (PhysHlth), whether an individual has difficulty walking or climbing stairs (DiffWalk), age, sex, income, and education level.\nThe primary purpose of the exploratory data analysis is to uncover patterns, relationships, and potential factors associated with diabetes. By examining the distribution of these variables and their relationships with diabetes status, we aim to identify key indicators that might contribute to the presence of diabetes. The ultimate goal of our modeling efforts is to develop a predictive model that can accurately classify individuals as having diabetes or not based on their health and demographic profiles. This model can be valuable for early detection and prevention strategies, allowing healthcare providers to identify at-risk individuals and implement targeted interventions to manage or mitigate the risk of diabetes."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "ST558 Final Project by Wenna Han - EDA",
    "section": "Data",
    "text": "Data\nread in data and check data structure.\n\n# read data with relative path\ndata &lt;- read.csv(\"./diabetes_binary_health_indicators_BRFSS2015.csv\") \n\n# view data structure\nstr(data)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : num  0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ...\n\n# check missingness\ncolSums(is.na(data))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThe data has been read successfully and no missing values were observed. However, many variables are coded numerically but they should be categorical data. Let’s convert those variables to factors with meaningful level names.\n\n# select variables that should be converted to 0/1 coded factors\nNo_factor&lt;-c(1:4, 6:14, 18) \n\n# Define the levels and labels\nlevels_0_1 &lt;- c(0, 1)\nlabels_no_yes &lt;- c(\"no\", \"yes\")\n\n# Convert the selected variables to factors with levels 0 and 1 and labels No and Yes\ndata[, No_factor] &lt;- lapply(data[, No_factor], function(x) {\n  factor(x, levels = levels_0_1, labels = labels_no_yes)\n})\n\n# Convert demographic variables to factors with labels\ndata &lt;- data |&gt;\n  mutate (Sex=factor(Sex, levels=c(0,1), labels=c(\"female\",\"male\")),\n          Age=factor(Age, levels=c(1:13), labels=c(\"18-24\",\"25-29\",\"30-34\",\n                                                   \"35-39\",\"40-44\",\"45-49\",\n                                                   \"50-54\",\"55-59\",\"60-64\",\n                                                   \"65-69\",\"70-74\",\"75-79\",\n                                                   \"80 or older\")),\n          Education=factor(Education, levels=c(1:6), \n                           labels=c(\"Never attended school or only kindergarten\",\n                           \"Elementary\", \"Some high school\", \"High school graduate\",\n                           \"Some college or technical school\", \"College graduate\")),\n          Income=factor(Income, levels=c(1:8), labels=c(\"less than $10,000\",\n                                                        \"$10,000 to less than $15,000\",\n                                                        \"$15,000 to less than $20,000\",\n                                                        \"$20,000 to less than $25,000\",\n                                                        \"$25,000 to less than $35,000\",\n                                                        \"$35,000 to less than $50,000\",\n                                                        \"$50,000 to less than $75,000\",\n                                                        \"$75,000 or more\")))\nstr(data) # check data structure\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck           : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke              : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"no\",\"yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AnyHealthcare       : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 2 2 2 2 2 2 2 ...\n $ NoDocbcCost         : Factor w/ 2 levels \"no\",\"yes\": 1 2 2 1 1 1 1 1 1 1 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex                 : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age                 : Factor w/ 13 levels \"18-24\",\"25-29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : Factor w/ 6 levels \"Never attended school or only kindergarten\",..: 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : Factor w/ 8 levels \"less than $10,000\",..: 3 1 8 6 4 8 7 4 1 3 ...\n\nsummary(data) # summarize data\n\n Diabetes_binary HighBP       HighChol     CholCheck         BMI       \n no :218334      no :144851   no :146089   no :  9470   Min.   :12.00  \n yes: 35346      yes:108829   yes:107591   yes:244210   1st Qu.:24.00  \n                                                        Median :27.00  \n                                                        Mean   :28.38  \n                                                        3rd Qu.:31.00  \n                                                        Max.   :98.00  \n                                                                       \n Smoker       Stroke       HeartDiseaseorAttack PhysActivity Fruits      \n no :141257   no :243388   no :229787           no : 61760   no : 92782  \n yes:112423   yes: 10292   yes: 23893           yes:191920   yes:160898  \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n                                                                         \n Veggies      HvyAlcoholConsump AnyHealthcare NoDocbcCost     GenHlth     \n no : 47839   no :239424        no : 12417    no :232326   Min.   :1.000  \n yes:205841   yes: 14256        yes:241263    yes: 21354   1st Qu.:2.000  \n                                                           Median :2.000  \n                                                           Mean   :2.511  \n                                                           3rd Qu.:3.000  \n                                                           Max.   :5.000  \n                                                                          \n    MentHlth         PhysHlth      DiffWalk         Sex              Age       \n Min.   : 0.000   Min.   : 0.000   no :211005   female:141974   60-64  :33244  \n 1st Qu.: 0.000   1st Qu.: 0.000   yes: 42675   male  :111706   65-69  :32194  \n Median : 0.000   Median : 0.000                                55-59  :30832  \n Mean   : 3.185   Mean   : 4.242                                50-54  :26314  \n 3rd Qu.: 2.000   3rd Qu.: 3.000                                70-74  :23533  \n Max.   :30.000   Max.   :30.000                                45-49  :19819  \n                                                                (Other):87744  \n                                      Education     \n Never attended school or only kindergarten:   174  \n Elementary                                :  4043  \n Some high school                          :  9478  \n High school graduate                      : 62750  \n Some college or technical school          : 69910  \n College graduate                          :107325  \n                                                    \n                          Income     \n $75,000 or more             :90385  \n $50,000 to less than $75,000:43219  \n $35,000 to less than $50,000:36470  \n $25,000 to less than $35,000:25883  \n $20,000 to less than $25,000:20135  \n $15,000 to less than $20,000:15994  \n (Other)                     :21594"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "ST558 Final Project by Wenna Han - EDA",
    "section": "Summarizations",
    "text": "Summarizations\nYou should then produce meaningful summary statistics and plots about the data you are working with (especially as it relates to your response). Although a best practice would be to split the data at hand into a training and testing set first, go ahead and do your EDA on the full data. Be sure to have a narrative about what you are exploring and what the summaries and graphs you created say about the relationships in your data.\n\n\n[1] 4\n\n\nClick here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#load-necessary-packages",
    "href": "EDA.html#load-necessary-packages",
    "title": "ST558 Final Project by Wenna Han - EDA",
    "section": "load necessary packages",
    "text": "load necessary packages\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(dplyr)"
  },
  {
    "objectID": "EDA.html#eda-summarizations",
    "href": "EDA.html#eda-summarizations",
    "title": "ST558 Final Project by Wenna Han - EDA",
    "section": "EDA & Summarizations",
    "text": "EDA & Summarizations\nSummarize data and do some exploratory data analysis. First, let’s focus on categorical variables:\n\n# Plotting function for categorical variables\nplot_categorical &lt;- function(var) {\n  ggplot(data, aes_string(x = var, fill = \"Diabetes_binary\")) +\n    geom_bar(position = \"dodge\") +\n    labs(title = paste(\"Distribution of\", var, \"by Diabetes_binary\"), x = var, y = \"Count\") +\n    theme_minimal()\n}\n\n# List of categorical variables\ncategorical_vars &lt;- c(\"HighBP\", \"HighChol\", \"CholCheck\", \"Smoker\", \"Stroke\",\n                      \"HeartDiseaseorAttack\", \"PhysActivity\", \"Fruits\", \n                      \"Veggies\", \"HvyAlcoholConsump\", \"AnyHealthcare\",\n                      \"NoDocbcCost\", \"DiffWalk\", \"Age\", \"Sex\", \"Education\", \"Income\")\n\n# Plot each categorical variable\nfor (var in categorical_vars) {\n  print(plot_categorical(var))\n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the above plots, we can find potential drivers of Diabetes_binary include:\n- HighBP:A higher proportion of individuals with diabetes have high blood pressure compared to those without diabetes.\n- HighChol: High cholesterol is more prevalent among those with diabetes.\n- CholCheck: Most individuals, both with and without diabetes, have had their cholesterol checked, but it is slightly higher among those with diabetes.\n- Stroke: Stroke is more common in individuals with diabetes compared to those without.\n- HeartDiseaseorAttack: Heart disease or attack is more common among individuals with diabetes.\n- PhysActivity: Physical activity is less common among individuals with diabetes.\n- Fruits: Lower consumption of fruits is associated with diabetes.\n- Veggies: Lower consumption of vegetables is associated with diabetes.\n- HvyAlcoholConsump: Heavy alcohol consumption is less common among individuals with diabetes.\n- DiffWalk: Difficulty walking is more common among individuals with diabetes.\n- Age: More elder people have diabetes than younger people.\n- Education: Among those with high education level, less portion of people got diabete.\n- Income: Diabetes is more common among individuals with low income.\nThen, let’s plot the numeric variables:\n\n# Plotting function for continuous variables\nplot_continuous &lt;- function(var) {\n  ggplot(data, aes_string(x = \"Diabetes_binary\", y = var, fill = \"Diabetes_binary\")) +\n    geom_boxplot() +\n    labs(title = paste(\"Boxplot of\", var, \"by Diabetes_binary\"), x = \"Diabetes_binary\", y = var) +\n    theme_minimal()\n}\n\n# List of continuous variables\ncontinuous_vars &lt;- c(\"BMI\", \"GenHlth\", \"MentHlth\", \"PhysHlth\")\n\n# Plot each continuous variable\nfor (var in continuous_vars) {\n  print(plot_continuous(var))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the above plots, we can find potential drivers of Diabetes_binary include:\n- BMI:Individuals with diabetes have higher BMI compared to those without diabetes.\n- GenHlth: General health is rated worse among individuals with diabetes.\n- MentHlth: Mental health is slightly poorer among individuals with diabetes.\n- PhysHlth: Physical health is poorer among individuals with diabetes.\n\nLastly, let’s see the correlation plot for all variables:\n\n# Convert categorical variables to numerical codes\ndata_numeric &lt;- data |&gt;\n  mutate_if(is.factor, as.numeric)\n\n# Compute the correlation matrix\ncorrelation_matrix &lt;- cor(data_numeric, use = \"complete.obs\")\n\n# Generate the correlation plot\ncorrplot(correlation_matrix, method = \"color\", \n         tl.cex = 0.8,\n         col = colorRampPalette(c(\"red\", \"white\", \"blue\"))(200),\n         type = \"upper\", \n         diag = FALSE)\n\n\n\n\n\n\n\n\nBased on all the above plots, the potential drivers of Diabetes_binary include:\n- High Blood Pressure (HighBP)\n- High Cholesterol (HighChol)\n- Body Mass Index (BMI)\n- Stroke\n- Heart Disease or Attack (HeartDiseaseorAttack)\n- Physical Activity (PhysActivity)\n- Fruits Consumption (Fruits)\n- Vegetables Consumption (Veggies)\n- Heavy Alcohol Consumption (HvyAlcoholConsump)\n- General Health (GenHlth)\n- Mental Health (MentHlth)\n- Physical Health (PhysHlth)\n- Difficulty Walking (DiffWalk)\n- Age\n- Education\n- Income\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "",
    "text": "In this study, we utilize the diabetes_binary_health_indicators_BRFSS2015.csv data (from https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) to investigate the relationships between diabetes status and a range of health-related variables.\nThe dataset comprises 218,334 individuals without diabetes and 35,346 individuals with diabetes, encompassing various demographic, behavioral, and health status indicators. The response variable is Diabetes_binary, which represent whether a person has diabetes or not. The data has other 21 variables. Based on the exploratory data analysis (EDA) results, the following variables are identified as potential predictors of Diabetes_binary:\n- High Blood Pressure (HighBP): whether an individual has high blood pressure.\n- High Cholesterol (HighChol): whether an individual has high cholesterol.\n- Body Mass Index (BMI): body mass index\n- Stroke: whether an individual has ever had a stroke.\n- Heart Disease or Attack (HeartDiseaseorAttack): whether an individual has ever had heart disease or a heart attack.\n- Physical Activity (PhysActivity): whether an individual has engaged in physical activity in the past month.\n- Fruits Consumption (Fruits): whether an individual consumes fruits daily.\n- Vegetables Consumption (Veggies): whether an individual consumes vegetables daily.\n- Heavy Alcohol Consumption (HvyAlcoholConsump): whether an individual engages in heavy alcohol consumption.\n- General Health (GenHlth): general health condition.\n- Mental Health (MentHlth): mental health condition.\n- Physical Health (PhysHlth): phsical health condition.\n- Difficulty Walking (DiffWalk): whether an individual has difficulty walking or climbing stairs.\n- Age\n- Education\n- Income\nThe goal of this project is to develop a predictive model that can accurately classify individuals as having diabetes or not based on their health and demographic profiles. This model can be valuable for early detection and prevention strategies, allowing healthcare providers to identify at-risk individuals and implement targeted interventions to manage or mitigate the risk of diabetes."
  },
  {
    "objectID": "Modeling.html#load-necessary-packages",
    "href": "Modeling.html#load-necessary-packages",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "load necessary packages",
    "text": "load necessary packages\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(dplyr)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.2.3\n\nlibrary(rpart)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:ranger':\n\n    importance\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin"
  },
  {
    "objectID": "Modeling.html#prepare-data",
    "href": "Modeling.html#prepare-data",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Prepare data",
    "text": "Prepare data\n\n# read data with relative path\ndata &lt;- read.csv(\"./diabetes_binary_health_indicators_BRFSS2015.csv\") \n\n# select variables that should be converted to 0/1 coded factors\nNo_factor&lt;-c(1:4, 6:14, 18) \n\n# Define the levels and labels\nlevels_0_1 &lt;- c(0, 1)\nlabels_no_yes &lt;- c(\"no\", \"yes\")\n\n# Convert the selected variables to factors with levels 0 and 1 and labels No and Yes\ndata[, No_factor] &lt;- lapply(data[, No_factor], function(x) {\n  factor(x, levels = levels_0_1, labels = labels_no_yes)\n})\n\n# Extract data for modeling\ndata &lt;- data |&gt;\n  select(-CholCheck, -Smoker, -AnyHealthcare, -NoDocbcCost, -Sex)\n\n# check data structure\nstr(data)\n\n'data.frame':   253680 obs. of  17 variables:\n $ Diabetes_binary     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP              : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol            : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 1 2 2 1 2 2 1 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Stroke              : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ HeartDiseaseorAttack: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity        : Factor w/ 2 levels \"no\",\"yes\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits              : Factor w/ 2 levels \"no\",\"yes\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies             : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump   : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ..."
  },
  {
    "objectID": "Modeling.html#split-data-7030",
    "href": "Modeling.html#split-data-7030",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Split data 70/30",
    "text": "Split data 70/30\n\nset.seed(5580728)\n\ntrainIndex &lt;- createDataPartition(data$Diabetes_binary, p = .7,\n                                  list = FALSE,\n                                  times = 1)\n\ntrain_set &lt;-  data[trainIndex, ]\ntest_set &lt;- data[-trainIndex, ]\n\n#check data\ndim(train_set)\n\n[1] 177577     17\n\ndim(test_set)\n\n[1] 76103    17"
  },
  {
    "objectID": "Modeling.html#use-logloss-as-the-metric-to-evaluate-models",
    "href": "Modeling.html#use-logloss-as-the-metric-to-evaluate-models",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Use logLoss as the metric to evaluate models",
    "text": "Use logLoss as the metric to evaluate models\nLog Loss is a performance metric for evaluating the accuracy of a classification model where the outcome is a probability value between 0 and 1. Specifically, for binary classification problems, Log Loss quantifies the uncertainty of the predictions made by the model. It penalizes false classifications, with a higher penalty for predictions that are confident but wrong.\nWe may prefer Log Loss over accuracy when we have a binary response variable since that accuracy only considers whether the prediction was correct or not, without accounting for the confidence of the prediction. Log Loss, on the other hand, takes into account the probability assigned to each class, penalizing confident incorrect predictions more than less confident ones. Also, in datasets with imbalanced classes, a high accuracy can be misleading if the model is biased towards the majority class. Log Loss mitigates this by penalizing misclassifications based on the predicted probabilities, providing a more accurate assessment of model performance across both classes."
  },
  {
    "objectID": "Modeling.html#implementation-of-log-loss-with-5-fold-cross-validation",
    "href": "Modeling.html#implementation-of-log-loss-with-5-fold-cross-validation",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Implementation of Log Loss with 5-Fold Cross-Validation",
    "text": "Implementation of Log Loss with 5-Fold Cross-Validation\nIn our study, we will use Log Loss with 5-fold cross-validation to evaluate and select the best predictive model for the Diabetes_binary variable. Cross-validation ensures that our model generalizes well to unseen data by assessing its performance on different subsets of the dataset. By setting up a grid of tuning parameters for each model, we aim to optimize the model’s hyperparameters to achieve the lowest possible Log Loss, ensuring robust and reliable predictions.\n\ntrctrl &lt;- trainControl(method = \"cv\", \n                       number = 5, \n                       summaryFunction = mnLogLoss, \n                       classProbs = TRUE)"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\nLogistic Regression is used for modeling a binary response variable, which means the dependent variable has two possible outcomes. Unlike linear regression, which predicts continuous outcomes, logistic regression predicts the probability of an event occurring. The logistic regression model estimates the probability p that an outcome Y is equal to 1 given a set of predictor variables X.\nWe choose it because logistic regression is designed for binary outcomes, making it ideal for our goal of predicting diabetes status (yes/no). The coefficients in logistic regression can be interpreted as the log odds of the outcome, providing insights into the relationship between predictor variables and the likelihood of diabetes. Also, it can handle both continuous and categorical predictor variables and can be extended to multi-class classification problems.\n\nModel 1: Basic Logistic Regression with all identified potential predictors\n\n# Model 1: Basic Logistic Regression with all identified potential predictors\nset.seed(123)\nlogit_model1 &lt;- train(Diabetes_binary ~ .,\n                      data = train_set,\n                      method = \"glm\",\n                      family = binomial,\n                      trControl = trctrl,\n                      metric = \"logLoss\"\n)\n\n\n\nModel 2: Logistic Regression with Stepwise Selection\nUse the stepwise selection to select predictors from the long list.\n\n# Model 2: Logistic Regression with Stepwise Selection\nset.seed(123)\nlogit_model2 &lt;- train(Diabetes_binary ~ .,\n                      data = train_set,\n                      method = \"glmStepAIC\",\n                      family = binomial,\n                      trControl = trctrl,\n                      metric = \"logLoss\"\n)\n\nStart:  AIC=91449.47\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    PhysActivityyes + Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + \n    GenHlth + MentHlth + PhysHlth + DiffWalkyes + Age + Education + \n    Income\n\n                          Df Deviance   AIC\n- PhysActivityyes          1    91417 91449\n&lt;none&gt;                          91415 91449\n- Veggiesyes               1    91421 91453\n- Fruitsyes                1    91425 91457\n- DiffWalkyes              1    91429 91461\n- Education                1    91431 91463\n- Strokeyes                1    91431 91463\n- MentHlth                 1    91435 91467\n- PhysHlth                 1    91461 91493\n- Income                   1    91467 91499\n- HeartDiseaseorAttackyes  1    91542 91574\n- HvyAlcoholConsumpyes     1    91680 91712\n- HighCholyes              1    92450 92482\n- Age                      1    92687 92719\n- HighBPyes                1    93074 93106\n- GenHlth                  1    94035 94067\n- BMI                      1    94113 94145\n\nStep:  AIC=91449.38\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + GenHlth + \n    MentHlth + PhysHlth + DiffWalkyes + Age + Education + Income\n\n                          Df Deviance   AIC\n&lt;none&gt;                          91417 91449\n- Veggiesyes               1    91423 91453\n- Fruitsyes                1    91428 91458\n- DiffWalkyes              1    91432 91462\n- Strokeyes                1    91433 91463\n- Education                1    91433 91463\n- MentHlth                 1    91437 91467\n- PhysHlth                 1    91462 91492\n- Income                   1    91469 91499\n- HeartDiseaseorAttackyes  1    91543 91573\n- HvyAlcoholConsumpyes     1    91682 91712\n- HighCholyes              1    92451 92481\n- Age                      1    92697 92727\n- HighBPyes                1    93076 93106\n- GenHlth                  1    94063 94093\n- BMI                      1    94140 94170\nStart:  AIC=91252.79\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    PhysActivityyes + Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + \n    GenHlth + MentHlth + PhysHlth + DiffWalkyes + Age + Education + \n    Income\n\n                          Df Deviance   AIC\n- PhysActivityyes          1    91219 91251\n&lt;none&gt;                          91219 91253\n- Veggiesyes               1    91225 91257\n- Fruitsyes                1    91228 91260\n- MentHlth                 1    91229 91261\n- Education                1    91236 91268\n- Strokeyes                1    91241 91273\n- DiffWalkyes              1    91245 91277\n- PhysHlth                 1    91262 91294\n- Income                   1    91265 91297\n- HeartDiseaseorAttackyes  1    91351 91383\n- HvyAlcoholConsumpyes     1    91491 91523\n- HighCholyes              1    92254 92286\n- Age                      1    92471 92503\n- HighBPyes                1    92919 92951\n- GenHlth                  1    93786 93818\n- BMI                      1    93986 94018\n\nStep:  AIC=91250.79\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + GenHlth + \n    MentHlth + PhysHlth + DiffWalkyes + Age + Education + Income\n\n                          Df Deviance   AIC\n&lt;none&gt;                          91219 91251\n- Veggiesyes               1    91225 91255\n- Fruitsyes                1    91228 91258\n- MentHlth                 1    91229 91259\n- Education                1    91236 91266\n- Strokeyes                1    91241 91271\n- DiffWalkyes              1    91245 91275\n- PhysHlth                 1    91262 91292\n- Income                   1    91265 91295\n- HeartDiseaseorAttackyes  1    91351 91381\n- HvyAlcoholConsumpyes     1    91491 91521\n- HighCholyes              1    92254 92284\n- Age                      1    92475 92505\n- HighBPyes                1    92919 92949\n- GenHlth                  1    93801 93831\n- BMI                      1    94001 94031\nStart:  AIC=91304.03\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    PhysActivityyes + Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + \n    GenHlth + MentHlth + PhysHlth + DiffWalkyes + Age + Education + \n    Income\n\n                          Df Deviance   AIC\n&lt;none&gt;                          91270 91304\n- PhysActivityyes          1    91273 91305\n- Veggiesyes               1    91279 91311\n- Fruitsyes                1    91281 91313\n- Education                1    91282 91314\n- DiffWalkyes              1    91286 91318\n- MentHlth                 1    91296 91328\n- Strokeyes                1    91296 91328\n- PhysHlth                 1    91302 91334\n- Income                   1    91329 91361\n- HeartDiseaseorAttackyes  1    91401 91433\n- HvyAlcoholConsumpyes     1    91546 91578\n- HighCholyes              1    92331 92363\n- Age                      1    92537 92569\n- HighBPyes                1    92927 92959\n- GenHlth                  1    93838 93870\n- BMI                      1    93924 93956\nStart:  AIC=91321.17\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    PhysActivityyes + Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + \n    GenHlth + MentHlth + PhysHlth + DiffWalkyes + Age + Education + \n    Income\n\n                          Df Deviance   AIC\n- PhysActivityyes          1    91288 91320\n&lt;none&gt;                          91287 91321\n- Fruitsyes                1    91291 91323\n- Veggiesyes               1    91293 91325\n- MentHlth                 1    91300 91332\n- DiffWalkyes              1    91303 91335\n- Education                1    91307 91339\n- Strokeyes                1    91316 91348\n- Income                   1    91319 91351\n- PhysHlth                 1    91327 91359\n- HeartDiseaseorAttackyes  1    91417 91449\n- HvyAlcoholConsumpyes     1    91547 91579\n- HighCholyes              1    92296 92328\n- Age                      1    92519 92551\n- HighBPyes                1    92978 93010\n- GenHlth                  1    93967 93999\n- BMI                      1    94031 94063\n\nStep:  AIC=91319.5\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + GenHlth + \n    MentHlth + PhysHlth + DiffWalkyes + Age + Education + Income\n\n                          Df Deviance   AIC\n&lt;none&gt;                          91288 91320\n- Fruitsyes                1    91292 91322\n- Veggiesyes               1    91294 91324\n- MentHlth                 1    91300 91330\n- DiffWalkyes              1    91304 91334\n- Education                1    91308 91338\n- Strokeyes                1    91316 91346\n- Income                   1    91320 91350\n- PhysHlth                 1    91327 91357\n- HeartDiseaseorAttackyes  1    91417 91447\n- HvyAlcoholConsumpyes     1    91547 91577\n- HighCholyes              1    92296 92326\n- Age                      1    92525 92555\n- HighBPyes                1    92978 93008\n- GenHlth                  1    93986 94016\n- BMI                      1    94052 94082\nStart:  AIC=91058.46\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    PhysActivityyes + Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + \n    GenHlth + MentHlth + PhysHlth + DiffWalkyes + Age + Education + \n    Income\n\n                          Df Deviance   AIC\n- PhysActivityyes          1    91026 91058\n&lt;none&gt;                          91024 91058\n- Fruitsyes                1    91030 91062\n- Veggiesyes               1    91033 91065\n- MentHlth                 1    91037 91069\n- DiffWalkyes              1    91038 91070\n- Strokeyes                1    91040 91072\n- Education                1    91042 91074\n- PhysHlth                 1    91067 91099\n- Income                   1    91072 91104\n- HeartDiseaseorAttackyes  1    91138 91170\n- HvyAlcoholConsumpyes     1    91331 91363\n- HighCholyes              1    92118 92150\n- Age                      1    92321 92353\n- HighBPyes                1    92655 92687\n- GenHlth                  1    93653 93685\n- BMI                      1    93877 93909\n\nStep:  AIC=91057.79\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + GenHlth + \n    MentHlth + PhysHlth + DiffWalkyes + Age + Education + Income\n\n                          Df Deviance   AIC\n&lt;none&gt;                          91026 91058\n- Fruitsyes                1    91032 91062\n- Veggiesyes               1    91035 91065\n- MentHlth                 1    91038 91068\n- DiffWalkyes              1    91040 91070\n- Strokeyes                1    91041 91071\n- Education                1    91044 91074\n- PhysHlth                 1    91068 91098\n- Income                   1    91074 91104\n- HeartDiseaseorAttackyes  1    91139 91169\n- HvyAlcoholConsumpyes     1    91332 91362\n- HighCholyes              1    92119 92149\n- Age                      1    92331 92361\n- HighBPyes                1    92657 92687\n- GenHlth                  1    93681 93711\n- BMI                      1    93906 93936\nStart:  AIC=114092.9\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    PhysActivityyes + Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + \n    GenHlth + MentHlth + PhysHlth + DiffWalkyes + Age + Education + \n    Income\n\n                          Df Deviance    AIC\n- PhysActivityyes          1   114060 114092\n&lt;none&gt;                         114059 114093\n- Veggiesyes               1   114068 114100\n- Fruitsyes                1   114069 114101\n- MentHlth                 1   114079 114111\n- Education                1   114079 114111\n- DiffWalkyes              1   114080 114112\n- Strokeyes                1   114085 114117\n- PhysHlth                 1   114110 114142\n- Income                   1   114118 114150\n- HeartDiseaseorAttackyes  1   114217 114249\n- HvyAlcoholConsumpyes     1   114403 114435\n- HighCholyes              1   115367 115399\n- Age                      1   115639 115671\n- HighBPyes                1   116143 116175\n- GenHlth                  1   117324 117356\n- BMI                      1   117487 117519\n\nStep:  AIC=114092\n.outcome ~ HighBPyes + HighCholyes + BMI + Strokeyes + HeartDiseaseorAttackyes + \n    Fruitsyes + Veggiesyes + HvyAlcoholConsumpyes + GenHlth + \n    MentHlth + PhysHlth + DiffWalkyes + Age + Education + Income\n\n                          Df Deviance    AIC\n&lt;none&gt;                         114060 114092\n- Veggiesyes               1   114069 114099\n- Fruitsyes                1   114070 114100\n- MentHlth                 1   114079 114109\n- Education                1   114081 114111\n- DiffWalkyes              1   114082 114112\n- Strokeyes                1   114086 114116\n- PhysHlth                 1   114110 114140\n- Income                   1   114119 114149\n- HeartDiseaseorAttackyes  1   114218 114248\n- HvyAlcoholConsumpyes     1   114404 114434\n- HighCholyes              1   115367 115397\n- Age                      1   115649 115679\n- HighBPyes                1   116144 116174\n- GenHlth                  1   117354 117384\n- BMI                      1   117517 117547\n\n\n\n\nModel 3: Logistic Regression with interaction Terms\nBased on EDA results, GenHealth is highly correlated with MentHlth as well as PhysHlth. Also, MentHlth and PhysHlth are correlated. Thus, in model 3, we removed the GenHealth and include teh interaction between MentHlth and PhysHlth.\n\n# Model 3: Logistic Regression with Lasso Regularization \nset.seed(123)\nlogit_model3 &lt;- train(Diabetes_binary ~ HighBP + HighChol +  BMI + \n                        Stroke + HeartDiseaseorAttack + PhysActivity +\n                        Fruits + Veggies + HvyAlcoholConsump + \n                        MentHlth*PhysHlth +  DiffWalk + Age + Education + Income,\n                      data = train_set,\n                      method = \"glm\",\n                      family = binomial,\n                      trControl = trctrl,\n                      metric = \"logLoss\"\n)\n\n\n\nCompare models\n\n# Summarize the results\nresults &lt;- resamples(list(\n  Model1 = logit_model1,\n  Model2 = logit_model2,\n  Model3 = logit_model3\n))\n\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: Model1, Model2, Model3 \nNumber of resamples: 5 \n\nlogLoss \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nModel1 0.3188326 0.3206710 0.3209088 0.3212773 0.3215952 0.3243789    0\nModel2 0.3188182 0.3206797 0.3209088 0.3212754 0.3215950 0.3243754    0\nModel3 0.3279369 0.3289130 0.3307234 0.3304687 0.3314235 0.3333465    0\n\n\nModel1 has the lowest log-loss, thus, we select logit_model2, which is:\n\nsummary(logit_model2)\n\n\nCall:\nNULL\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7838  -0.5348  -0.3171  -0.1842   3.4002  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -6.6275133  0.0715371 -92.644  &lt; 2e-16 ***\nHighBPyes                0.7869715  0.0175817  44.761  &lt; 2e-16 ***\nHighCholyes              0.5810615  0.0162062  35.854  &lt; 2e-16 ***\nBMI                      0.0624992  0.0010688  58.476  &lt; 2e-16 ***\nStrokeyes                0.1551061  0.0300089   5.169 2.36e-07 ***\nHeartDiseaseorAttackyes  0.2666938  0.0210560  12.666  &lt; 2e-16 ***\nFruitsyes               -0.0522218  0.0162415  -3.215   0.0013 ** \nVeggiesyes              -0.0576772  0.0189184  -3.049   0.0023 ** \nHvyAlcoholConsumpyes    -0.7867895  0.0464048 -16.955  &lt; 2e-16 ***\nGenHlth                  0.5421920  0.0096734  56.050  &lt; 2e-16 ***\nMentHlth                -0.0044386  0.0010106  -4.392 1.12e-05 ***\nPhysHlth                -0.0065651  0.0009316  -7.047 1.83e-12 ***\nDiffWalkyes              0.0949783  0.0200751   4.731 2.23e-06 ***\nAge                      0.1272244  0.0032667  38.945  &lt; 2e-16 ***\nEducation               -0.0379208  0.0082647  -4.588 4.47e-06 ***\nIncome                  -0.0319620  0.0041495  -7.703 1.33e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 114060  on 177561  degrees of freedom\nAIC: 114092\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "Modeling.html#classification-tree-models",
    "href": "Modeling.html#classification-tree-models",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Classification Tree Models",
    "text": "Classification Tree Models\nA classification tree is a type of decision tree used for categorizing a dependent variable into distinct classes. It is a supervised learning technique that partitions the dataset into subsets based on the value of input features, creating a tree-like model of decisions. Each internal node in the tree represents a “test” on an attribute (e.g., whether an attribute is less than or greater than a certain value), each branch represents the outcome of the test, and each leaf node represents a class label (decision) or distribution of class labels.\nWe try classification trees because trees are easy to understand and visualize, making them useful for explaining model decisions. Also, classification trees can capture non-linear relationships between features and the target variable. It do not require normalization or standardization of features. Moreover, it can help identify the most important features in the dataset.\n\n# classification tree\nset.seed(123)\nclassification_tree_model &lt;- train(Diabetes_binary ~ .,\n                            data = train_set,\n                            method = \"rpart\",\n                            trControl = trctrl,\n                            tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)),\n                            metric = \"logLoss\")\nclassification_tree_model\n\nCART \n\n177577 samples\n    16 predictor\n     2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.4128608\n  0.001  0.3546310\n  0.002  0.3547351\n  0.003  0.3547576\n  0.004  0.3651691\n  0.005  0.3651691\n  0.006  0.3945187\n  0.007  0.3945187\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\nlogLoss was used to select the optimal model using the smallest value. The final value used for the model was cp = 0.001."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Random Forest",
    "text": "Random Forest\nA random forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is an extension of bagging (Bootstrap Aggregating) and leverages the power of multiple models to improve accuracy and control over-fitting.\nWe want to try random forest model because it can achieve better accuracy than individual decision trees by averaging multiple trees. Also, random forests are less likely to overfit compared to individual trees.\n\nset.seed(123)\nrandom_forest_model &lt;- train(Diabetes_binary ~ .,\n                             data = train_set,\n                             method = \"ranger\",\n                             trControl = trctrl,\n                             tuneGrid = expand.grid(mtry = c(2, 4, 6),\n                                                    splitrule = \"extratrees\",\n                                                    min.node.size = c(1, 5, 10)),\n                             metric = \"logLoss\",\n                             num.trees = 100)\nrandom_forest_model\n\nRandom Forest \n\n177577 samples\n    16 predictor\n     2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  mtry  min.node.size  logLoss  \n  2      1             0.3263554\n  2      5             0.3268752\n  2     10             0.3264590\n  4      1             0.3237531\n  4      5             0.3222660\n  4     10             0.3210288\n  6      1             0.3355324\n  6      5             0.3298554\n  6     10             0.3258248\n\nTuning parameter 'splitrule' was held constant at a value of extratrees\nlogLoss was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 4, splitrule = extratrees\n and min.node.size = 10.\n\n\nlogLoss was used to select the optimal model using the smallest value. The final values used for the model were mtry = 4, splitrule = extratrees and min.node.size = 10."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "ST558 Final Project by Wenna Han - Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nNow we have three best models (logit_model1, classification_tree_model, random_forest_model). Now compare all three models on the test set and declare an overall winner.\n\n# Predict probabilities for the test set\nlogit_probs &lt;- predict(logit_model1, newdata = test_set, type = \"prob\")\ntree_probs &lt;- predict(classification_tree_model, newdata = test_set, type = \"prob\")\nrf_probs &lt;- predict(random_forest_model, newdata = test_set, type = \"prob\")\n\n# Define a function to calculate log loss\nlogLoss &lt;- function(pred_probs, actual) {\n  actual &lt;- ifelse(actual == levels(actual)[1], 0, 1)\n  -mean(actual * log(pred_probs[, 2]) + (1 - actual) * log(pred_probs[, 1]))\n}\n\n# Calculate log loss for each model\nlogit_log_loss &lt;- logLoss(logit_probs, test_set$Diabetes_binary)\ntree_log_loss &lt;- logLoss(tree_probs, test_set$Diabetes_binary)\nrf_log_loss &lt;- logLoss(rf_probs, test_set$Diabetes_binary)\n\n# Print log loss values\nlist(Logit_LogLoss = logit_log_loss,\n     Classification_Tree_LogLoss = tree_log_loss,\n     Random_Forest_LogLoss = rf_log_loss)\n\n$Logit_LogLoss\n[1] 0.3217086\n\n$Classification_Tree_LogLoss\n[1] 0.3554793\n\n$Random_Forest_LogLoss\n[1] 0.3209674\n\n\nThe random forest model has the smallest log loss, thus, the random forest model is the winner!"
  }
]